# -*- coding: utf-8 -*-

"""RAG Chain with LangChain ConversationBufferMemory - Production ready"""

import os
import json
import torch

from datetime import datetime
from typing import Optional, Dict, List, Any

import streamlit as st

from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from openai import OpenAI

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import (
    HumanMessage, 
    AIMessage, 
    ToolMessage, 
    SystemMessage
)
from langchain_core.tools import tool
from langchain_core.runnables import RunnableLambda

# ============================================================================
# CONFIGURATION
# ============================================================================

INDEX_NAME = "youtube-qa-index"
TOP_K = 5

# ============================================================================
# GLOBAL STATE
# ============================================================================

_initialized = False
retriever = None
pc = None
index = None
llm = None
llm_with_tools = None
rag_agent_chain = None

# âœ¨ NEW: LangChain ConversationBufferMemory
memory = None  # Initialized in initialize_chain()

# ============================================================================
# ENVIRONMENT SETUP (works with Streamlit secrets)
# ============================================================================

def _setup_env():
    """Load environment variables - works in Colab and Streamlit"""
    os.environ.setdefault("LANGCHAIN_TRACING_V2", "true")
    os.environ.setdefault("LANGCHAIN_ENDPOINT", "https://api.smith.langchain.com")
    os.environ.setdefault("LANGCHAIN_PROJECT", "memory-and-tools-rag-agent")

# ============================================================================
# RETRIEVER - CACHED FOR PERFORMANCE
# ============================================================================

@st.cache_resource
def get_retriever():
    """Load embedding model - cached for performance"""
    print("ðŸ“¥ Loading SentenceTransformer model (768 dims)...")
    device = "cpu"  # Force CPU for Streamlit Cloud
    retriever = SentenceTransformer(
        "sentence-transformers/all-mpnet-base-v2",  # 768 dims
        device=device
    )
    print("âœ… SentenceTransformer loaded (768 dims)")
    return retriever

# ============================================================================
# TOOLS
# ============================================================================

@tool
def calculator(expression: str) -> str:
    """Evaluate a mathematical expression. Example: '2 + 2 * 5' or '10 / 3'"""
    try:
        result = eval(expression)
        return f"Result: {result}"
    except Exception as e:
        return f"Error: {str(e)}"

@tool
def get_current_time() -> str:
    """Get the current date and time."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

@tool
def word_count(text: str) -> str:
    """Count the number of words in a given text."""
    count = len(text.split())
    return f"Word count: {count}"

@tool
def convert_case(text: str, case_type: str) -> str:
    """Convert text to uppercase, lowercase, or title case. 
    case_type options: 'upper', 'lower', 'title'"""
    if case_type == "upper":
        return text.upper()
    elif case_type == "lower":
        return text.lower()
    elif case_type == "title":
        return text.title()
    else:
        return f"Error: Unknown case type '{case_type}'. Use 'upper', 'lower', or 'title'."

@tool
def estimate_targets(weight_kg: float, sex: str, activity: str, goal: str) -> str:
    """
    Estimate daily calories and protein for a user.
    Args:
        weight_kg: Body weight in kilograms.
        sex: 'male' or 'female'.
        activity: 'sedentary', 'light', 'moderate', 'active'.
        goal: 'maintain', 'lose', 'gain'.
    """
    # Simple kcal-per-kg factors by activity level (approximate maintenance)
    factor = {
        "sedentary": 28,   # low movement
        "light": 31,       # light exercise
        "moderate": 34,    # 3â€“5 sessions/week
        "active": 37       # hard training/manual work
    }.get(activity, 31)

    maintenance_cals = weight_kg * factor

    if goal == "lose":
        target_cals = maintenance_cals - 400
        goal_text = "weight loss"
    elif goal == "gain":
        target_cals = maintenance_cals + 400
        goal_text = "muscle gain"
    else:
        target_cals = maintenance_cals
        goal_text = "weight maintenance"

    # Protein: 1.6â€“2.2g/kg body weight for active lifters
    protein_low = weight_kg * 1.6
    protein_high = weight_kg * 2.2

    return (
        f"Estimated daily targets for {goal_text}:\n"
        f"- Calories: {int(target_cals)} kcal per day\n"
        f"- Protein: {protein_low:.1f}â€“{protein_high:.1f} g per day\n"
        "These are simplified estimates and should be adjusted for age, body composition, and training volume."
    )

tools = [calculator, get_current_time, word_count, convert_case, estimate_targets]

# ============================================================================
# RAG RETRIEVAL
# ============================================================================

def retrieve_pinecone_context(query: str, top_k: int = TOP_K) -> Dict:
    """Query Pinecone with embedded version of user query"""
    if retriever is None or index is None:
        return {"matches": []}

    try:
        xq = retriever.encode(query).tolist()
        res = index.query(vector=xq, top_k=top_k, include_metadata=True, timeout=10)
        return res
    except Exception as e:
        print(f"âš ï¸ Pinecone retrieval error: {e}")
        return {"matches": []}

def context_string_from_matches(matches: List) -> str:
    """Build context string from Pinecone matches"""
    parts = []
    for m in matches:
        meta = m.get("metadata", {})
        passage = meta.get("text") or meta.get("passage_text") or ""
        if passage:
            parts.append(passage)
    return "\n\n".join(parts)

# ============================================================================
# CHAIN RUNNABLES - WITH LANGCHAIN MEMORY
# ============================================================================

def _build_messages(inputs: dict) -> dict:
    """STEP 1: Build messages with RAG context and memory"""
    user_message = inputs["user_message"]

    # RAG: retrieve context from Pinecone
    pinecone_result = retrieve_pinecone_context(user_message)
    context = context_string_from_matches(pinecone_result.get("matches", []))

    messages = []

    # System prompt
    messages.append(
        SystemMessage(
            content=(
                "You are a friendly, evidence-based personal trainer and RAG assistant. "
                "Your goals are to: (1) give safe, practical fitness advice; "
                "(2) tailor suggestions to the user's level and goals; "
                "(3) clearly explain reasoning in simple language.\n\n"
                "Always use the retrieved knowledge base context when it is relevant.\n\n"
                "Tool usage rules:\n"
                "- If the user asks for general arithmetic or numeric computations (e.g. 75 * 22, percentages), "
                "you MUST call the `calculator` tool.\n"
                "- If the user asks for word counts, you MUST call the `word_count` tool.\n"
                "- If the user asks for case conversion, you MUST call the `convert_case` tool.\n"
                "- If the user asks for the current time or date, you MUST call the `get_current_time` tool.\n"
                "- If the user asks for calorie or protein targets, daily macro targets, or bodyweight-based "
                "nutrition targets (e.g. 'I am 75 kg, moderate activity, want to lose weight â€“ what should my "
                "calories and protein be?'), you MUST call ONLY the `estimate_targets` tool and NOT the "
                "`calculator` tool.\n\n"
                "When you use any tool, explicitly mention in your explanation that you used that tool, and base "
                "your answer directly on the tool's output instead of estimating."
            )
        )
    )

    # âœ¨ NEW: Load conversation history from LangChain memory
    history = memory.load_memory_variables({})
    if history.get("history"):
        # Parse history string back into messages if needed
        # The memory returns a formatted string, so we add it as context
        messages.append(HumanMessage(content=f"Previous conversation:\n{history['history']}"))

    # Current user message
    messages.append(HumanMessage(content=user_message))

    # Optional RAG context message
    if context:
        messages.append(
            HumanMessage(
                content=f"ðŸ“š Relevant context from knowledge base:\n{context}"
            )
        )

    return {
        "messages": messages,
        "rag_context": context,
    }

def _first_llm_call(state: dict) -> dict:
    """STEP 2: Call LLM with tools enabled (decision step)"""
    messages = state["messages"]
    first_response = llm_with_tools.invoke(messages)
    return {
        **state,
        "first_response": first_response,
    }

def _run_tools_if_needed(state: dict) -> dict:
    """STEP 3: Execute tools if LLM requested them"""
    first_response = state["first_response"]
    messages = state["messages"]

    # Extract tool calls
    tool_calls = getattr(first_response, "tool_calls", None)
    if not tool_calls and hasattr(first_response, "additional_kwargs"):
        tool_calls = first_response.additional_kwargs.get("tool_calls")

    # If no tool calls, propagate state forward
    if not tool_calls:
        return {
            **state,
            "messages_with_tools": messages,
        }

    # Execute tools
    tool_results_messages = []
    for call in tool_calls:
        tool_name = call.get("name") or call.get("function", {}).get("name")
        raw_args = (
            call.get("args")
            or call.get("arguments")
            or call.get("function", {}).get("arguments", {})
        )

        # Parse args if JSON string
        if isinstance(raw_args, str):
            try:
                tool_args = json.loads(raw_args)
            except Exception:
                tool_args = {}
        else:
            tool_args = raw_args or {}

        tool_id = call.get("id", "tool_call")

        # Find matching tool
        matching = [t for t in tools if t.name == tool_name]
        if not matching:
            result_text = f"Tool '{tool_name}' not found."
        else:
            try:
                result_text = matching[0].invoke(tool_args)
            except Exception as e:
                result_text = f"Error in tool '{tool_name}': {e}"

        tool_results_messages.append(
            ToolMessage(content=str(result_text), tool_call_id=tool_id)
        )

    # Build new message list with tools results
    messages_with_tools = messages + [first_response] + tool_results_messages

    return {
        **state,
        "messages_with_tools": messages_with_tools,
    }

def _final_llm_call(state: dict) -> dict:
    """STEP 4: Call plain LLM for final answer"""
    messages_with_tools = state["messages_with_tools"]
    final_response = llm.invoke(messages_with_tools)

    return {
        "final_response": final_response,
        "rag_context": state.get("rag_context", ""),
    }

# ============================================================================
# INITIALIZATION
# ============================================================================

def initialize_chain():
    """Initialize all components - call once at startup"""
    global _initialized, retriever, pc, index, llm, llm_with_tools, rag_agent_chain, memory

    if _initialized:
        return

    _setup_env()
    print("ðŸ”§ Initializing RAG chain with LangChain memory...")

    # Get cached retriever (768 dims - matches your Pinecone index)
    retriever = get_retriever()

    # Pinecone
    pinecone_key = os.getenv("PINECONE_API_KEY")
    if not pinecone_key:
        raise ValueError("PINECONE_API_KEY not set")

    pc = Pinecone(api_key=pinecone_key)
    index = pc.Index(INDEX_NAME)
    print(f"âœ… Connected to Pinecone index: {INDEX_NAME} (768 dims)")

    # LangChain LLM
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    llm_with_tools = llm.bind_tools(tools)
    print(f"âœ… Loaded {len(tools)} tools and LLM")

    # âœ¨ NEW: Initialize LangChain ConversationBufferMemory
    memory = ConversationBufferMemory(
        ai_prefix="Assistant",
        human_prefix="User",
        return_messages=True  # Return as Message objects, not strings
    )
    print("âœ… LangChain ConversationBufferMemory initialized")

    # Build chain
    build_messages = RunnableLambda(_build_messages)
    first_llm_call = RunnableLambda(_first_llm_call)
    run_tools_if_needed = RunnableLambda(_run_tools_if_needed)
    final_llm_call = RunnableLambda(_final_llm_call)

    rag_agent_chain = (
        RunnableLambda(lambda user_message: {"user_message": user_message})
        | build_messages
        | first_llm_call
        | run_tools_if_needed
        | final_llm_call
    )

    print("âœ… RAG chain initialized with LangChain memory and ready")
    _initialized = True

# ============================================================================
# MAIN CHAT FUNCTION
# ============================================================================

def chat_with_rag_and_tools(user_message: str) -> str:
    """Main chat function - call this with user input. Memory is auto-saved."""
    global memory

    if not _initialized:
        raise RuntimeError("Chain not initialized. Call initialize_chain() first.")

    try:
        result = rag_agent_chain.invoke(user_message)
        final_response = result["final_response"]
        response_text = final_response.content

        # âœ¨ NEW: Save to LangChain memory automatically
        memory.save_context(
            {"input": user_message},
            {"output": response_text}
        )
        print(f"âœ… Memory saved. Total messages in memory: {len(memory.buffer)}")

        return response_text

    except Exception as e:
        print(f"âŒ Error in chat: {e}")
        raise

# ============================================================================
# MEMORY UTILITY FUNCTIONS (optional, for advanced use)
# ============================================================================

def get_memory_summary() -> Dict:
    """Get current memory state - useful for debugging"""
    if memory is None:
        return {"status": "Memory not initialized"}

    history_vars = memory.load_memory_variables({})
    return {
        "history": history_vars.get("history", ""),
        "message_count": len(memory.chat_memory.messages) if hasattr(memory, "chat_memory") else 0,
    }

def clear_memory():
    """Clear all conversation memory"""
    global memory
    if memory is not None:
        memory.clear()
        print("ðŸ—‘ï¸ Memory cleared")

def get_memory_messages_list() -> List[Dict]:
    """Get memory as a list of dicts (for Streamlit session state sync)"""
    if memory is None or not hasattr(memory, "chat_memory"):
        return []

    messages = []
    for msg in memory.chat_memory.messages:
        if isinstance(msg, HumanMessage):
            messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            messages.append({"role": "assistant", "content": msg.content})

    return messages